{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "16\n",
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "print (env.action_space.n)\n",
    "print (env.observation_space.n)\n",
    "#print(env.action)\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table.shape)\n",
    "\n",
    "num_episodes = 20000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFFF\n",
    "# FHFH\n",
    "# FFFH\n",
    "# HFFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Q-learning algorithm\n",
    "\n",
    "rewards_all_episodes = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode): \n",
    "\n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "                     \n",
    "                     \n",
    "    # Exploration rate decay   \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "                         \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode) \n",
    "       \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.37 s for 7000 iterations at convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.05300000000000004\n",
      "2000 :  0.19600000000000015\n",
      "3000 :  0.4110000000000003\n",
      "4000 :  0.5600000000000004\n",
      "5000 :  0.6600000000000005\n",
      "6000 :  0.6580000000000005\n",
      "7000 :  0.6970000000000005\n",
      "8000 :  0.6980000000000005\n",
      "9000 :  0.6770000000000005\n",
      "10000 :  0.6730000000000005\n",
      "11000 :  0.6960000000000005\n",
      "12000 :  0.6860000000000005\n",
      "13000 :  0.6920000000000005\n",
      "14000 :  0.6580000000000005\n",
      "15000 :  0.6800000000000005\n",
      "16000 :  0.7020000000000005\n",
      "17000 :  0.6740000000000005\n",
      "18000 :  0.6620000000000005\n",
      "19000 :  0.7020000000000005\n",
      "20000 :  0.6830000000000005\n",
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.53279062 0.51265299 0.51802076 0.51756081]\n",
      " [0.34712812 0.36372618 0.23754269 0.49698779]\n",
      " [0.40067772 0.39890217 0.39507192 0.48092604]\n",
      " [0.35965761 0.25225665 0.25128814 0.4649693 ]\n",
      " [0.56022095 0.38467624 0.30459866 0.33402818]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.17403314 0.19474543 0.34450645 0.13676069]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44754507 0.35888055 0.30559006 0.60269988]\n",
      " [0.48695353 0.62957267 0.52657909 0.37481878]\n",
      " [0.56349749 0.4891017  0.51540297 0.35513148]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.51499365 0.56067136 0.77115287 0.43530673]\n",
      " [0.76158612 0.91124037 0.74186585 0.72725238]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "policy: \n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 3]\n",
      " [0 0 2 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "    \n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)\n",
    "print(\"\")\n",
    "\n",
    "# Create a deterministic policy using the optimal value function\n",
    "policy = np.zeros([state_space_size, action_space_size])\n",
    "for s in range(state_space_size):\n",
    "    # One step lookahead to find the best action for this state\n",
    "    #A = one_step_lookahead(s, q_table)\n",
    "    best_action = np.argmax(q_table[s])\n",
    "    # Always take the best action\n",
    "    policy[s, best_action] = 1.0\n",
    "\n",
    "print(\"policy: \")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(q_table, axis=1), [4,4]))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "64\n",
      "(64, 4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "print (env.action_space.n)\n",
    "print (env.observation_space.n)\n",
    "#print(env.action)\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table.shape)\n",
    "\n",
    "num_episodes = 50000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Q-learning algorithm\n",
    "\n",
    "rewards_all_episodes = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode): \n",
    "\n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "                     \n",
    "                     \n",
    "    # Exploration rate decay   \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "                         \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.0\n",
      "2000 :  0.0\n",
      "3000 :  0.001\n",
      "4000 :  0.002\n",
      "5000 :  0.011000000000000003\n",
      "6000 :  0.028000000000000018\n",
      "7000 :  0.037000000000000026\n",
      "8000 :  0.05600000000000004\n",
      "9000 :  0.05300000000000004\n",
      "10000 :  0.08700000000000006\n",
      "11000 :  0.08600000000000006\n",
      "12000 :  0.11900000000000009\n",
      "13000 :  0.12000000000000009\n",
      "14000 :  0.1590000000000001\n",
      "15000 :  0.19400000000000014\n",
      "16000 :  0.18300000000000013\n",
      "17000 :  0.20300000000000015\n",
      "18000 :  0.21900000000000017\n",
      "19000 :  0.25400000000000017\n",
      "20000 :  0.2850000000000002\n",
      "21000 :  0.32900000000000024\n",
      "22000 :  0.2960000000000002\n",
      "23000 :  0.3120000000000002\n",
      "24000 :  0.3790000000000003\n",
      "25000 :  0.34300000000000025\n",
      "26000 :  0.36200000000000027\n",
      "27000 :  0.3930000000000003\n",
      "28000 :  0.3840000000000003\n",
      "29000 :  0.4240000000000003\n",
      "30000 :  0.44700000000000034\n",
      "31000 :  0.4300000000000003\n",
      "32000 :  0.46700000000000036\n",
      "33000 :  0.45000000000000034\n",
      "34000 :  0.43900000000000033\n",
      "35000 :  0.5010000000000003\n",
      "36000 :  0.5040000000000003\n",
      "37000 :  0.4910000000000004\n",
      "38000 :  0.44500000000000034\n",
      "39000 :  0.5110000000000003\n",
      "40000 :  0.5390000000000004\n",
      "41000 :  0.45400000000000035\n",
      "42000 :  0.5120000000000003\n",
      "43000 :  0.4940000000000004\n",
      "44000 :  0.5060000000000003\n",
      "45000 :  0.5050000000000003\n",
      "46000 :  0.5330000000000004\n",
      "47000 :  0.5530000000000004\n",
      "48000 :  0.5250000000000004\n",
      "49000 :  0.5070000000000003\n",
      "50000 :  0.5570000000000004\n",
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[3.97915117e-01 4.08459821e-01 4.01325853e-01 4.28975886e-01]\n",
      " [4.09218047e-01 3.88102884e-01 4.40570932e-01 4.14893615e-01]\n",
      " [4.31257554e-01 4.15344077e-01 4.48346994e-01 4.12913488e-01]\n",
      " [4.29497399e-01 4.38992847e-01 4.65359301e-01 4.48634547e-01]\n",
      " [4.58780109e-01 4.60651211e-01 4.79254138e-01 4.69496051e-01]\n",
      " [4.75696184e-01 4.90301903e-01 5.06533806e-01 4.94044573e-01]\n",
      " [5.07738234e-01 5.16259328e-01 5.23214217e-01 5.16730303e-01]\n",
      " [5.26632268e-01 5.24404051e-01 5.24218834e-01 5.23101913e-01]\n",
      " [3.70356383e-01 3.78228761e-01 3.74164575e-01 4.26109319e-01]\n",
      " [3.94050403e-01 3.92757848e-01 3.78216709e-01 4.33100112e-01]\n",
      " [3.96605839e-01 3.69251542e-01 4.00621765e-01 4.40828794e-01]\n",
      " [3.06915521e-01 3.23181009e-01 2.89388079e-01 4.56429650e-01]\n",
      " [4.30875838e-01 4.33874255e-01 4.40410024e-01 4.74635233e-01]\n",
      " [4.67493601e-01 4.70585434e-01 4.96240724e-01 4.69961288e-01]\n",
      " [5.22254426e-01 5.25904730e-01 5.29665064e-01 5.23126966e-01]\n",
      " [5.32716497e-01 5.37843605e-01 5.32531999e-01 5.28812318e-01]\n",
      " [3.55599524e-01 3.53938530e-01 3.50059684e-01 3.77642476e-01]\n",
      " [3.50379191e-01 3.39208397e-01 3.47017084e-01 3.90637363e-01]\n",
      " [3.31323014e-01 2.11148774e-01 2.65455464e-01 2.84057259e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.70256446e-01 2.28003119e-01 4.21056853e-01 2.69116114e-01]\n",
      " [3.11316953e-01 2.80785921e-01 3.44637343e-01 4.86620400e-01]\n",
      " [5.32604429e-01 5.39253563e-01 5.42452956e-01 5.32423928e-01]\n",
      " [5.51351083e-01 5.73083671e-01 5.56899563e-01 5.52731259e-01]\n",
      " [3.29456482e-01 3.25130321e-01 3.22474853e-01 3.43841081e-01]\n",
      " [3.04980555e-01 2.91451300e-01 3.00213511e-01 3.41764794e-01]\n",
      " [2.26919760e-01 2.22610878e-01 2.37856043e-01 2.98518640e-01]\n",
      " [7.68937654e-02 9.70372281e-02 7.01671187e-02 2.77480851e-01]\n",
      " [3.21390156e-01 1.36614239e-01 1.53294552e-01 1.46024334e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.06200383e-01 4.27423711e-01 5.50257803e-01 4.14354360e-01]\n",
      " [5.73422699e-01 6.41958060e-01 5.87668134e-01 5.70301589e-01]\n",
      " [2.59817607e-01 2.56629265e-01 2.59809954e-01 2.76129409e-01]\n",
      " [2.03079201e-01 1.57854319e-01 2.00408746e-01 2.75580175e-01]\n",
      " [1.87873832e-01 9.57785424e-02 1.03244026e-01 1.15662887e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.78528359e-01 1.48578512e-01 2.99746409e-01 1.69162949e-01]\n",
      " [1.78754691e-01 3.41704770e-01 2.28431809e-01 1.94284157e-01]\n",
      " [2.43645801e-01 3.97008883e-01 3.19029357e-01 4.70659714e-01]\n",
      " [6.44753096e-01 6.13869927e-01 6.93882165e-01 6.04693577e-01]\n",
      " [1.82926661e-01 5.17271669e-02 1.27504309e-01 1.01440255e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.10320609e-03 2.29433078e-02 7.76413213e-02 2.35366147e-02]\n",
      " [1.27992071e-01 1.08169039e-01 7.45910735e-02 2.12327210e-01]\n",
      " [2.78035880e-01 1.98716906e-01 1.49803638e-01 1.62428312e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.17805124e-01 5.05872358e-01 7.69237183e-01 3.91914899e-01]\n",
      " [3.97682617e-02 1.58128121e-02 2.22490592e-02 7.88787552e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.87790083e-05 1.51485300e-02 4.38328338e-04 3.65096114e-03]\n",
      " [9.28647144e-03 1.81326349e-03 6.39838514e-03 1.72733514e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.71150795e-01 1.22492539e-01 1.27860148e-01 6.51589041e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.56374573e-01 7.02508863e-01 8.79890254e-01 5.51972365e-01]\n",
      " [4.19462804e-03 1.96247376e-03 1.49389123e-04 6.27659924e-02]\n",
      " [3.41765081e-02 7.24503893e-04 1.73562386e-07 8.74080634e-05]\n",
      " [2.03308414e-02 8.31028496e-04 2.22160134e-03 8.53427753e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [9.28324268e-02 1.20619844e-01 2.13224839e-01 4.01994409e-02]\n",
      " [3.38430516e-01 3.40468479e-01 4.20164754e-01 3.46128158e-01]\n",
      " [2.28194898e-01 7.88045235e-01 2.76620881e-01 3.56459383e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "policy: \n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[3 2 2 2 2 2 2 0]\n",
      " [3 3 3 3 3 2 2 1]\n",
      " [3 3 0 0 2 3 2 1]\n",
      " [3 3 3 3 0 0 2 1]\n",
      " [3 3 0 0 2 1 3 2]\n",
      " [0 0 0 2 3 0 0 2]\n",
      " [3 0 1 3 0 0 0 2]\n",
      " [3 0 0 0 2 2 1 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# http://deeplizard.com/learn/video/HGeI30uATws\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "    \n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)\n",
    "print(\"\")\n",
    "\n",
    "# Create a deterministic policy using the optimal value function\n",
    "policy = np.zeros([state_space_size, action_space_size])\n",
    "for s in range(state_space_size):\n",
    "    # One step lookahead to find the best action for this state\n",
    "    #A = one_step_lookahead(s, q_table)\n",
    "    best_action = np.argmax(q_table[s])\n",
    "    # Always take the best action\n",
    "    policy[s, best_action] = 1.0\n",
    "\n",
    "print(\"policy: \")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(q_table, axis=1), [8,8]))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "Training finished.\n",
      "\n",
      "[-2.25806326 -1.97170038 -2.26115506 -2.24294086 -5.12566784 -7.48627941]\n",
      "(500, 6)\n",
      "Wall time: 7.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\"\"\"\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "print (q_table.shape)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "#for i in range(1, 100001):\n",
    "for i in range(1, 10001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")\n",
    "\n",
    "print (q_table[328])\n",
    "print (q_table.shape)\n",
    "\"\"\"\n",
    "\n",
    "#\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "#total_epochs, total_penalties = 0, 0\n",
    "#episodes = 100\n",
    "\n",
    "#for _ in range(episodes):\n",
    "#    state = env.reset()\n",
    "#    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "#    done = False\n",
    "    \n",
    "#    while not done:\n",
    "#        action = np.argmax(q_table[state])\n",
    "#        state, reward, done, info = env.step(action)\n",
    "\n",
    "#        if reward == -10:\n",
    "#            penalties += 1\n",
    "\n",
    "#        epochs += 1\n",
    "\n",
    "#    total_penalties += penalties\n",
    "#    total_epochs += epochs\n",
    "\n",
    "#print(f\"Results after {episodes} episodes:\")\n",
    "#print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "#print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
